{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Embeddings_and_Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmol0720/Word-Embedding/blob/master/Word_Embeddings_and_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6pAHP8zaV8i",
        "colab_type": "code",
        "outputId": "5bdcdf9b-0bb3-40b4-c1dc-4e4b5fc5d924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "! wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'\n",
        "! wget 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "! unzip glove.6B.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-02 07:39:37--  https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84188 (82K) [application/x-httpd-php]\n",
            "Saving to: ‘sentiment labelled sentences.zip.2’\n",
            "\n",
            "sentiment labelled  100%[===================>]  82.21K   298KB/s    in 0.3s    \n",
            "\n",
            "2019-11-02 07:39:38 (298 KB/s) - ‘sentiment labelled sentences.zip.2’ saved [84188/84188]\n",
            "\n",
            "--2019-11-02 07:39:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-11-02 07:39:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-11-02 07:39:41--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  2.00MB/s    in 6m 30s  \n",
            "\n",
            "2019-11-02 07:46:11 (2.11 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n",
            "Archive:  sentiment labelled sentences.zip\n",
            "replace sentiment labelled sentences/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: mv: cannot move '/content/sentiment labelled sentences/' to '/content/data/sentiment labelled sentences': Directory not empty\n",
            "mkdir: cannot create directory ‘data/sentiment_analysis’: File exists\n",
            "mv: cannot move 'data' to a subdirectory of itself, 'data/sentiment_analysis/data'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCZj67ZwHPRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a4c6dd9c-461d-426d-d9cb-445112dabf10"
      },
      "source": [
        "! unzip 'sentiment labelled sentences.zip'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sentiment labelled sentences.zip\n",
            "replace sentiment labelled sentences/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: sentiment labelled sentences/.DS_Store  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._.DS_Store  \n",
            "  inflating: sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._imdb_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/readme.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._readme.txt  \n",
            "  inflating: sentiment labelled sentences/yelp_labelled.txt  \n",
            "  inflating: __MACOSX/._sentiment labelled sentences  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEFxtPaCIVX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv /content/'sentiment labelled sentences'/ /content/data1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFw2W9dyIdaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir data1/sentiment_analysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIAtPjf5Ih9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb51ee6b-edc6-4a2e-c419-2836b61f2a53"
      },
      "source": [
        "! mv data data1/sentiment_analysis"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot move 'data1' to a subdirectory of itself, 'data1/sentiment_analysis/data1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZHk_gDr0Skn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv glove.6B.zip data1/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vrKGudy0ZUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "076f52f4-8983-4b0c-87e4-de1c3252fa7d"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " amazon_cells_labelled.txt   readme.txt\n",
            " data1\t\t\t     sample_data\n",
            " glove.6B.zip.1\t\t    'sentiment labelled sentences.zip'\n",
            " glove.6B.zip.2\t\t    'sentiment labelled sentences.zip.1'\n",
            " imdb_labelled.txt\t    'sentiment labelled sentences.zip.2'\n",
            " __MACOSX\t\t     yelp_labelled.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_VNivxbaqhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "11476281-28e7-41c8-be4c-9fec7c5723e4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "filepath_dict = {'yelp':   'data1/yelp_labelled.txt',\n",
        "                 'amazon': 'data1/amazon_cells_labelled.txt',\n",
        "                 'imdb':   'data1/imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.head())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            sentence  label source\n",
            "0                           Wow... Loved this place.      1   yelp\n",
            "1                                 Crust is not good.      0   yelp\n",
            "2          Not tasty and the texture was just nasty.      0   yelp\n",
            "3  Stopped by during the late May bank holiday of...      1   yelp\n",
            "4  The selection on the menu was great and so wer...      1   yelp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGJYD4F2bCAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "   sentences, y, test_size=0.25, random_state=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbtZPiUWb9Qe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7ecd4f5b-d51d-4f97-c73b-7272ddc11157"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "sent=vectorizer.fit(sentences_train)\n",
        "\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "X_train\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<750x1714 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 7368 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6MVxEiKOETM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c7e45eed-99a1-45cb-93b1-464ab48d1947"
      },
      "source": [
        "print(sent.get_feature_names())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '10', '100', '11', '12', '15', '1979', '20', '2007', '23', '30', '35', '40', '40min', '4ths', '5lb', '70', '90', '99', 'about', 'above', 'absolute', 'absolutely', 'accident', 'accommodations', 'accordingly', 'accountant', 'ache', 'acknowledged', 'across', 'actual', 'actually', 'added', 'affordable', 'after', 'afternoon', 'again', 'ago', 'ahead', 'airport', 'ala', 'albondigas', 'all', 'allergy', 'almonds', 'almost', 'alone', 'also', 'although', 'always', 'am', 'amazing', 'ambiance', 'ambience', 'amount', 'ample', 'an', 'and', 'angry', 'annoying', 'another', 'any', 'anything', 'anytime', 'anyway', 'anyways', 'apart', 'apologize', 'app', 'appalling', 'apparently', 'appealing', 'appetite', 'appetizer', 'appetizers', 'apple', 'are', 'area', 'aren', 'around', 'array', 'arrived', 'arrives', 'arriving', 'article', 'as', 'ask', 'asked', 'asking', 'assure', 'at', 'ate', 'atmosphere', 'atrocious', 'attached', 'attack', 'attention', 'attentive', 'attitudes', 'auju', 'authentic', 'average', 'avocado', 'avoid', 'avoided', 'away', 'awesome', 'awful', 'awkward', 'awkwardly', 'ayce', 'az', 'baba', 'baby', 'bachi', 'back', 'bacon', 'bad', 'bagels', 'bakery', 'baklava', 'bamboo', 'banana', 'bar', 'bare', 'barely', 'bargain', 'bars', 'bartender', 'bartenders', 'basically', 'bathroom', 'bathrooms', 'batter', 'bay', 'bbq', 'be', 'bean', 'beans', 'beat', 'beateous', 'beautiful', 'beautifully', 'because', 'beef', 'been', 'beer', 'before', 'begin', 'behind', 'being', 'believe', 'belly', 'below', 'besides', 'best', 'better', 'between', 'beyond', 'big', 'bigger', 'biggest', 'bill', 'binge', 'bird', 'biscuit', 'biscuits', 'bit', 'bitches', 'bite', 'blah', 'bland', 'blandest', 'blanket', 'block', 'bloddy', 'bloodiest', 'bloody', 'blow', 'blown', 'blue', 'boba', 'boiled', 'bone', 'book', 'boot', 'boring', 'both', 'bother', 'bottom', 'bouchon', 'bought', 'bowl', 'box', 'boy', 'boyfriend', 'boys', 'bread', 'break', 'breakfast', 'brick', 'bring', 'brings', 'brisket', 'brought', 'brownish', 'brunch', 'bruschetta', 'brushfire', 'bucks', 'buffet', 'buffets', 'building', 'buldogis', 'bunch', 'burger', 'burgers', 'burned', 'burrittos', 'bus', 'business', 'businesses', 'busy', 'but', 'butter', 'buying', 'by', 'bye', 'caballero', 'cafe', 'café', 'cake', 'cakes', 'calamari', 'calligraphy', 'callings', 'came', 'camelback', 'can', 'cannoli', 'cannot', 'cant', 'cape', 'capers', 'car', 'carbs', 'care', 'caring', 'carly', 'carpaccio', 'cart', 'cartel', 'cash', 'cashew', 'cashier', 'casino', 'caterpillar', 'caught', 'cavier', 'changing', 'char', 'charcoal', 'charge', 'cheated', 'check', 'checked', 'cheek', 'cheese', 'cheeseburger', 'cheesecurds', 'chef', 'chefs', 'chewy', 'chicken', 'chickens', 'chinese', 'chip', 'chipolte', 'chipotle', 'chips', 'chocolate', 'choose', 'choux', 'chow', 'christmas', 'circumstances', 'claimed', 'class', 'classic', 'classics', 'clean', 'close', 'closed', 'club', 'clue', 'cocktail', 'cocktails', 'coconut', 'cod', 'coffee', 'cold', 'college', 'color', 'combination', 'combo', 'combos', 'come', 'comfortable', 'coming', 'common', 'companions', 'company', 'complain', 'complaints', 'complete', 'completely', 'compliments', 'con', 'concept', 'concern', 'conclusion', 'condiment', 'consider', 'considering', 'constructed', 'contain', 'convenient', 'cooked', 'cooking', 'cool', 'corn', 'corporation', 'correct', 'correction', 'costco', 'cotta', 'could', 'couldn', 'count', 'couple', 'couples', 'coupons', 'course', 'court', 'cover', 'covered', 'covers', 'cow', 'crab', 'cranberry', 'crawfish', 'crazy', 'cream', 'creamy', 'crema', 'crepe', 'crisp', 'crispy', 'croutons', 'crowds', 'crumby', 'crust', 'crusty', 'crêpe', 'cuisine', 'curry', 'customer', 'customers', 'customize', 'cut', 'cute', 'daily', 'damn', 'dark', 'daughter', 'day', 'dead', 'deal', 'dealing', 'decent', 'decided', 'decor', 'decorated', 'dedicated', 'deep', 'deeply', 'def', 'definately', 'definitely', 'degree', 'del', 'delicate', 'delicioso', 'delicious', 'deliciously', 'delight', 'delightful', 'delights', 'delish', 'deliver', 'denny', 'descriptions', 'deserves', 'desired', 'despicable', 'despite', 'dessert', 'desserts', 'deuchebaggery', 'devine', 'did', 'didn', 'die', 'different', 'dime', 'dine', 'dining', 'dinner', 'dinners', 'dipping', 'dirt', 'dirty', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disapppointment', 'disaster', 'disbelief', 'disgrace', 'disgraceful', 'disgust', 'disgusting', 'dish', 'dishes', 'dispenser', 'disrespected', 'diverse', 'do', 'does', 'dog', 'doing', 'dollars', 'don', 'done', 'dont', 'door', 'dos', 'double', 'doubt', 'douchey', 'dough', 'doughy', 'down', 'downright', 'downside', 'downtown', 'drawing', 'dreamed', 'drenched', 'dressed', 'dressing', 'dried', 'driest', 'drink', 'drinking', 'drinks', 'dripping', 'drive', 'drunk', 'dry', 'duck', 'due', 'duo', 'during', 'dusted', 'dylan', 'each', 'eat', 'eaten', 'eating', 'edible', 'edinburgh', 'editing', 'eel', 'eew', 'egg', 'eggplant', 'eggs', 'either', 'elk', 'else', 'elsewhere', 'email', 'employee', 'employees', 'empty', 'end', 'ended', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enough', 'ensued', 'enthusiastic', 'entire', 'entrees', 'equally', 'especially', 'establishment', 'etc', 'ethic', 'eve', 'even', 'evening', 'events', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'excalibur', 'exceeding', 'excellent', 'excuse', 'expanded', 'expect', 'expectations', 'expected', 'expensive', 'experience', 'experienced', 'experiencing', 'exquisite', 'extensive', 'extra', 'extraordinary', 'extremely', 'fabulous', 'fact', 'fail', 'fails', 'fairly', 'falafels', 'falling', 'familiar', 'family', 'fancy', 'fantastic', 'far', 'fare', 'fast', 'fat', 'fav', 'favorite', 'fear', 'feel', 'feeling', 'feels', 'fell', 'fella', 'felt', 'few', 'fiancé', 'filet', 'fill', 'fillet', 'filling', 'final', 'finally', 'find', 'fine', 'finger', 'finish', 'fireball', 'firehouse', 'first', 'fish', 'five', 'flair', 'flat', 'flavor', 'flavored', 'flavorful', 'flavorless', 'flavors', 'flavourful', 'flirting', 'flop', 'flower', 'fluffy', 'fly', 'fo', 'focused', 'folks', 'fondue', 'food', 'foods', 'foot', 'for', 'forever', 'forgetting', 'forth', 'forty', 'forward', 'found', 'four', 'francisco', 'frenchman', 'fresh', 'fried', 'friend', 'friendly', 'friends', 'fries', 'from', 'front', 'frozen', 'fruit', 'fry', 'fs', 'full', 'fun', 'funny', 'further', 'furthermore', 'ganoush', 'garden', 'garlic', 'gas', 'gave', 'gc', 'gem', 'generic', 'generous', 'genuinely', 'get', 'getting', 'giant', 'girlfriend', 'give', 'given', 'giving', 'glad', 'glass', 'gloves', 'go', 'goat', 'godfathers', 'going', 'gold', 'golden', 'gone', 'good', 'google', 'gordon', 'got', 'gotten', 'gourmet', 'grain', 'grandmother', 'gratuity', 'grease', 'greasy', 'great', 'greatest', 'greedy', 'greek', 'green', 'greens', 'greeted', 'grill', 'grilled', 'gringos', 'gristle', 'gross', 'grossed', 'ground', 'group', 'groups', 'grow', 'guacamole', 'guess', 'guests', 'guy', 'guys', 'gyro', 'gyros', 'ha', 'had', 'hadn', 'half', 'halibut', 'hamburger', 'han', 'hand', 'handed', 'handled', 'handling', 'handmade', 'hands', 'happened', 'happier', 'happy', 'hard', 'has', 'hasn', 'hated', 'haunt', 'have', 'having', 'he', 'head', 'heads', 'healthy', 'heard', 'heart', 'hearts', 'heat', 'heimer', 'help', 'helped', 'helpful', 'her', 'here', 'hi', 'high', 'highlighted', 'highly', 'hilarious', 'him', 'hip', 'hiro', 'his', 'hit', 'hits', 'hole', 'home', 'homemade', 'honeslty', 'honest', 'honestly', 'honor', 'hooked', 'hope', 'hopes', 'horrible', 'host', 'hostess', 'hot', 'hottest', 'hour', 'hours', 'house', 'how', 'however', 'huevos', 'huge', 'human', 'humiliated', 'hummus', 'hungry', 'hurry', 'husband', 'hut', 'ians', 'ice', 'iced', 'if', 'ignore', 'ignored', 'im', 'imagination', 'imaginative', 'imagine', 'immediately', 'impeccable', 'impressed', 'impressive', 'in', 'incredible', 'indian', 'indicate', 'indoor', 'inexpensive', 'inflate', 'informative', 'ingredients', 'insanely', 'inside', 'inspired', 'instantly', 'instead', 'insulted', 'insults', 'interesting', 'into', 'ironman', 'is', 'isn', 'it', 'italian', 'item', 'its', 'itself', 'jalapeno', 'jenni', 'job', 'joey', 'join', 'joint', 'joke', 'joy', 'judge', 'judging', 'juice', 'juries', 'just', 'kabuki', 'kept', 'key', 'khao', 'kiddos', 'kids', 'kind', 'know', 'known', 'lack', 'lacked', 'lacking', 'ladies', 'lady', 'large', 'largely', 'larger', 'last', 'lastly', 'later', 'lawyers', 'least', 'leather', 'leave', 'leaves', 'left', 'leftover', 'legit', 'legs', 'lemon', 'let', 'letdown', 'letting', 'lettuce', 'level', 'life', 'light', 'lighter', 'lightly', 'like', 'liked', 'limited', 'lined', 'list', 'listed', 'literally', 'little', 'live', 'lived', 'll', 'lobster', 'location', 'long', 'look', 'looked', 'lordy', 'lost', 'lot', 'lots', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'low', 'lox', 'luke', 'lukewarm', 'lunch', 'mac', 'macarons', 'made', 'madhouse', 'madison', 'magazine', 'maine', 'mains', 'make', 'managed', 'management', 'manager', 'mandalay', 'many', 'margaritas', 'maria', 'market', 'marrow', 'mary', 'may', 'maybe', 'mayo', 'me', 'meal', 'meals', 'means', 'meat', 'meatballs', 'meats', 'mediocre', 'mediterranean', 'medium', 'meet', 'meh', 'mein', 'mellow', 'melt', 'melted', 'memory', 'mention', 'menu', 'menus', 'mesquite', 'mess', 'metro', 'mexican', 'mgm', 'middle', 'might', 'mile', 'milk', 'milkshake', 'min', 'mind', 'minutes', 'mirage', 'miss', 'missed', 'missing', 'mistake', 'mixed', 'mmmm', 'moist', 'mom', 'money', 'monster', 'months', 'moods', 'more', 'mortified', 'mostly', 'mouth', 'mouthful', 'movies', 'moz', 'mozzarella', 'much', 'muffin', 'multi', 'multiple', 'mushroom', 'mushrooms', 'music', 'must', 'my', 'myself', 'nachos', 'name', 'nan', 'nasty', 'nay', 'neat', 'need', 'needed', 'needless', 'needs', 'negligent', 'neighborhood', 'neither', 'never', 'new', 'next', 'nice', 'nicest', 'night', 'ninja', 'no', 'nobu', 'noca', 'non', 'none', 'not', 'note', 'nothing', 'now', 'nude', 'nutshell', 'nyc', 'obviously', 'occasions', 'of', 'off', 'offers', 'officially', 'oh', 'ohhh', 'oil', 'ok', 'old', 'omelets', 'omg', 'on', 'once', 'one', 'ones', 'onion', 'only', 'opened', 'operation', 'opinion', 'opportunity', 'options', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'otherwise', 'our', 'ourselves', 'out', 'outrageously', 'outshining', 'outside', 'outstanding', 'oven', 'over', 'overall', 'overcooked', 'overhaul', 'overpriced', 'overwhelmed', 'owner', 'owners', 'oysters', 'pace', 'pack', 'packed', 'paid', 'palate', 'pale', 'palm', 'pan', 'pancake', 'pancakes', 'panna', 'paper', 'par', 'part', 'particular', 'parties', 'party', 'passed', 'past', 'pasta', 'pastry', 'patio', 'patty', 'pay', 'peach', 'peanut', 'peanuts', 'pears', 'pecan', 'penne', 'people', 'pepper', 'perfect', 'perfection', 'perfectly', 'performed', 'perhaps', 'perpared', 'person', 'personable', 'personally', 'petrified', 'philadelphia', 'pho', 'phoenix', 'piano', 'picture', 'pictures', 'piece', 'pile', 'pink', 'pita', 'pizza', 'pizzas', 'place', 'places', 'plain', 'plate', 'playing', 'pleasant', 'please', 'pleased', 'plus', 'pneumatic', 'point', 'poisoning', 'polite', 'poop', 'poor', 'poorly', 'pop', 'pork', 'portion', 'portions', 'possible', 'potato', 'potatoes', 'poured', 'powdered', 'power', 'prefer', 'prepare', 'preparing', 'presentation', 'pretty', 'price', 'priced', 'prices', 'pricey', 'pricing', 'prime', 'privileged', 'probably', 'problem', 'professional', 'profiterole', 'profound', 'promise', 'prompt', 'promptly', 'pros', 'proven', 'provides', 'public', 'pucks', 'pulled', 'pumpkin', 'puree', 'puréed', 'putting', 'qualified', 'quality', 'quantity', 'quick', 'quickly', 'quite', 'ramsey', 'ranch', 'rancheros', 'rapidly', 'rare', 'rarely', 'raspberry', 'rate', 'rated', 'rather', 'rating', 'ratio', 'rave', 'raving', 'ravoli', 're', 'read', 'readers', 'real', 'realized', 'really', 'reason', 'reasonable', 'reasonably', 'received', 'receives', 'recent', 'recently', 'recommend', 'recommendation', 'recommended', 'red', 'redeeming', 'refill', 'refreshing', 'refried', 'register', 'regular', 'regularly', 'relationship', 'relax', 'relaxed', 'relleno', 'relocated', 'remember', 'reminded', 'reminds', 'replenished', 'requested', 'rest', 'restaraunt', 'restaurant', 'restaurants', 'return', 'returned', 'returning', 'review', 'reviewer', 'reviews', 'revisiting', 'rge', 'ri', 'rib', 'ribeye', 'rice', 'rich', 'ridiculous', 'right', 'rings', 'rinse', 'ripped', 'roast', 'roasted', 'rock', 'roll', 'rolls', 'room', 'round', 'rowdy', 'rubber', 'rude', 'rudely', 'running', 'rushed', 'ryan', 'sad', 'sadly', 'saffron', 'saganaki', 'said', 'salad', 'salads', 'salmon', 'sals', 'salsa', 'salt', 'san', 'sandwich', 'sangria', 'sashimi', 'sat', 'satisfied', 'sauce', 'sauces', 'sause', 'saving', 'say', 'saying', 'says', 'scallop', 'scene', 'screams', 'screwed', 'seafood', 'seasonal', 'seasoned', 'seasoning', 'seat', 'seated', 'seating', 'second', 'section', 'see', 'seemed', 'seems', 'seen', 'selection', 'selections', 'send', 'sense', 'sergeant', 'seriously', 'serivce', 'serve', 'served', 'server', 'servers', 'serves', 'service', 'services', 'serving', 'set', 'setting', 'sever', 'several', 'sewer', 'sexy', 'shall', 'sharply', 'shawarrrrrrma', 'she', 'shirt', 'shocked', 'shoe', 'shoots', 'shop', 'shops', 'shots', 'should', 'shouldn', 'shower', 'shrimp', 'sick', 'side', 'sides', 'sign', 'signs', 'similar', 'simple', 'simply', 'since', 'single', 'sitting', 'six', 'skimp', 'slaw', 'sliced', 'slices', 'slow', 'small', 'smaller', 'smashburger', 'smeared', 'smelled', 'smells', 'smoke', 'smooth', 'so', 'soggy', 'soi', 'solidify', 'some', 'somehow', 'someone', 'somethat', 'something', 'somewhat', 'son', 'songs', 'soon', 'soooo', 'sooooo', 'sore', 'sorely', 'sorry', 'sound', 'soundtrack', 'soup', 'soups', 'sour', 'southwest', 'spaghetti', 'special', 'specials', 'speedy', 'spend', 'spends', 'spice', 'spices', 'spicier', 'spicy', 'spinach', 'sporting', 'spot', 'spots', 'spotty', 'spring', 'staff', 'stale', 'star', 'stars', 'started', 'station', 'stay', 'stayed', 'staying', 'steak', 'steaks', 'steiners', 'step', 'stepped', 'sticks', 'still', 'stinks', 'stir', 'stomach', 'stood', 'stop', 'stopped', 'strange', 'strawberry', 'street', 'stretch', 'strike', 'strings', 'strip', 'struck', 'struggle', 'stuff', 'stuffed', 'stupid', 'style', 'styrofoam', 'sub', 'subpar', 'subway', 'such', 'suck', 'sucked', 'sucker', 'sucks', 'suffers', 'sugar', 'sugary', 'suggest', 'suggestions', 'summarize', 'summary', 'summer', 'sun', 'sunday', 'sunglasses', 'super', 'sure', 'surprise', 'surprised', 'sushi', 'sweet', 'swung', 'table', 'tables', 'taco', 'tacos', 'tailored', 'take', 'talk', 'talking', 'tapas', 'tartar', 'tartare', 'taste', 'tasted', 'tasteless', 'tasty', 'tater', 'tea', 'teamwork', 'teeth', 'tell', 'temp', 'ten', 'tender', 'tenders', 'terrible', 'terrific', 'texture', 'thai', 'than', 'thanks', 'that', 'thats', 'the', 'theft', 'their', 'them', 'themselves', 'then', 'there', 'these', 'they', 'thick', 'thin', 'thing', 'things', 'think', 'thinly', 'third', 'thirty', 'this', 'those', 'though', 'thought', 'three', 'thrilled', 'thru', 'thumbs', 'tigerlilly', 'time', 'times', 'tip', 'tiramisu', 'to', 'toast', 'toasted', 'today', 'told', 'tolerance', 'tomato', 'tongue', 'tonight', 'too', 'took', 'top', 'tops', 'toro', 'total', 'totally', 'tots', 'touch', 'touched', 'towards', 'town', 'tracked', 'tragedy', 'transcendant', 'trap', 'treat', 'treated', 'tried', 'trimmed', 'trip', 'trippy', 'trips', 'truffle', 'truly', 'try', 'trying', 'tummy', 'tuna', 'turn', 'tv', 'twice', 'two', 'typical', 'unbelievable', 'unbelievably', 'under', 'undercooked', 'underwhelming', 'unexperienced', 'unfortunately', 'unhealthy', 'uninspired', 'unique', 'unless', 'unsatisfying', 'until', 'untoasted', 'unwelcome', 'unwrapped', 'up', 'update', 'upgrading', 'uploaded', 'us', 'use', 'used', 'usual', 'vacant', 'vain', 'valley', 'value', 'vanilla', 've', 'veal', 'vegas', 'vegetables', 'vegetarian', 'velvet', 'ventilation', 'venture', 'venturing', 'venue', 'verge', 'version', 'very', 'via', 'vibe', 'vinaigrette', 'vinegrette', 'violinists', 'visit', 'vodka', 'voted', 'waaaaaayyyyyyyyyy', 'wagyu', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'waitresses', 'walked', 'wall', 'walls', 'want', 'wanted', 'wants', 'warm', 'warmer', 'warnings', 'was', 'wash', 'wasn', 'waste', 'wasted', 'wasting', 'watch', 'watched', 'water', 'watered', 'wave', 'way', 'ways', 'wayyy', 'we', 'weak', 'website', 'week', 'weekend', 'weekly', 'weird', 'well', 'went', 'were', 'weren', 'what', 'whatsoever', 'whelm', 'when', 'whenever', 'where', 'whether', 'which', 'while', 'white', 'who', 'whole', 'why', 'wide', 'wife', 'will', 'wine', 'wings', 'winner', 'wish', 'with', 'without', 'witnessed', 'won', 'wonderful', 'wontons', 'word', 'work', 'worker', 'workers', 'working', 'world', 'worse', 'worst', 'worth', 'would', 'wouldn', 'wound', 'wow', 'writing', 'wrong', 'ya', 'yama', 'yeah', 'year', 'yellow', 'yellowtail', 'yet', 'you', 'your', 'yucky', 'yukon', 'yum', 'yummy', 'zero']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Q98WlAN26x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "265ce22b-2c63-4cb9-9937-93550dc8a4fd"
      },
      "source": [
        "print(X_test.toarray())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_izFz-iZcc6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression(solver= 'lbfgs')\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQg1UPKFcfxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    sentences = df_source['sentence'].values\n",
        "    y = df_source['label'].values\n",
        "\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(sentences_train)\n",
        "    X_train = vectorizer.transform(sentences_train)\n",
        "    X_test  = vectorizer.transform(sentences_test)\n",
        "\n",
        "    classifier = LogisticRegression(solver= 'lbfgs')\n",
        "    classifier.fit(X_train, y_train)\n",
        "    score = classifier.score(X_test, y_test)\n",
        "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfU-L8MPc-db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZnmCALqc-pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "input_dim = X_train.shape[1]  # Number of features\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbQikDWNc_aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVZ7M1TzdUZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    \n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwaPLCDEdp9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(sentences_train[2])\n",
        "print(X_train[2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKv4DZsfd5C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in ['the', 'all', 'me', 'you']:\n",
        "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDSW2j_8d1Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "print(X_train[0, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs-vX_2pEe6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88YN0snneGZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 50\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim= vocab_size, \n",
        "                           output_dim= embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuRR5Q2keJjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDYp6kHreO5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim,\n",
        "                           \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIyAlWIBysvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-cZ36A-vtTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! head -n 1 glove.6B.50d.txt | cut -c-50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbd7JBRSvzxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "    'data/glove.6B.50d.txt',\n",
        "    tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2jA-SW1GBIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "! ls cd data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLWLk3nrwFI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre-trained word embeddings\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUbjP-fWwMua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trainable embeddings layer\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim,\n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen,\n",
        "                           trainable=True))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMmBxdHQwmj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ZcnpCywyfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[5000], \n",
        "                  embedding_dim=[50],\n",
        "                  maxlen=[100])\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = 'data/output.txt'\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "for source, frame in df.groupby('source'):\n",
        "    print('Running grid search for data set :', source)\n",
        "    sentences = df['sentence'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    # Tokenize words\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "    # Adding 1 because of reserved 0 index\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Pad sequences with zeros\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "    model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate testing set\n",
        "    test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "    # Save and evaluate results\n",
        "    prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
        "    if prompt.lower() not in {'y', 'true', 'yes'}:\n",
        "        break\n",
        "    with open(output_file, 'a') as f:\n",
        "        s = ('Running {} data set\\nBest Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "        output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "        print(output_string)\n",
        "        f.write(output_string)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}