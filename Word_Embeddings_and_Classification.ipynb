{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Embeddings_and_Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmol0720/Word-Embedding/blob/master/Word_Embeddings_and_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6pAHP8zaV8i",
        "colab_type": "code",
        "outputId": "5bdcdf9b-0bb3-40b4-c1dc-4e4b5fc5d924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "! wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'\n",
        "! wget 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "! unzip glove.6B.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-02 07:39:37--  https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84188 (82K) [application/x-httpd-php]\n",
            "Saving to: ‘sentiment labelled sentences.zip.2’\n",
            "\n",
            "sentiment labelled  100%[===================>]  82.21K   298KB/s    in 0.3s    \n",
            "\n",
            "2019-11-02 07:39:38 (298 KB/s) - ‘sentiment labelled sentences.zip.2’ saved [84188/84188]\n",
            "\n",
            "--2019-11-02 07:39:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-11-02 07:39:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-11-02 07:39:41--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  2.00MB/s    in 6m 30s  \n",
            "\n",
            "2019-11-02 07:46:11 (2.11 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n",
            "Archive:  sentiment labelled sentences.zip\n",
            "replace sentiment labelled sentences/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: mv: cannot move '/content/sentiment labelled sentences/' to '/content/data/sentiment labelled sentences': Directory not empty\n",
            "mkdir: cannot create directory ‘data/sentiment_analysis’: File exists\n",
            "mv: cannot move 'data' to a subdirectory of itself, 'data/sentiment_analysis/data'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCZj67ZwHPRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a4c6dd9c-461d-426d-d9cb-445112dabf10"
      },
      "source": [
        "! unzip 'sentiment labelled sentences.zip'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sentiment labelled sentences.zip\n",
            "replace sentiment labelled sentences/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: sentiment labelled sentences/.DS_Store  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._.DS_Store  \n",
            "  inflating: sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._imdb_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/readme.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._readme.txt  \n",
            "  inflating: sentiment labelled sentences/yelp_labelled.txt  \n",
            "  inflating: __MACOSX/._sentiment labelled sentences  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEFxtPaCIVX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv /content/'sentiment labelled sentences'/ /content/data1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFw2W9dyIdaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir data1/sentiment_analysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIAtPjf5Ih9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb51ee6b-edc6-4a2e-c419-2836b61f2a53"
      },
      "source": [
        "! mv data data1/sentiment_analysis"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot move 'data1' to a subdirectory of itself, 'data1/sentiment_analysis/data1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZHk_gDr0Skn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv glove.6B.zip data1/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vrKGudy0ZUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "076f52f4-8983-4b0c-87e4-de1c3252fa7d"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " amazon_cells_labelled.txt   readme.txt\n",
            " data1\t\t\t     sample_data\n",
            " glove.6B.zip.1\t\t    'sentiment labelled sentences.zip'\n",
            " glove.6B.zip.2\t\t    'sentiment labelled sentences.zip.1'\n",
            " imdb_labelled.txt\t    'sentiment labelled sentences.zip.2'\n",
            " __MACOSX\t\t     yelp_labelled.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_VNivxbaqhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "11476281-28e7-41c8-be4c-9fec7c5723e4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "filepath_dict = {'yelp':   'data1/yelp_labelled.txt',\n",
        "                 'amazon': 'data1/amazon_cells_labelled.txt',\n",
        "                 'imdb':   'data1/imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.head())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            sentence  label source\n",
            "0                           Wow... Loved this place.      1   yelp\n",
            "1                                 Crust is not good.      0   yelp\n",
            "2          Not tasty and the texture was just nasty.      0   yelp\n",
            "3  Stopped by during the late May bank holiday of...      1   yelp\n",
            "4  The selection on the menu was great and so wer...      1   yelp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGJYD4F2bCAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "   sentences, y, test_size=0.25, random_state=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbtZPiUWb9Qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train)\n",
        "\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "X_train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_izFz-iZcc6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression(solver= 'lbfgs')\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQg1UPKFcfxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    sentences = df_source['sentence'].values\n",
        "    y = df_source['label'].values\n",
        "\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(sentences_train)\n",
        "    X_train = vectorizer.transform(sentences_train)\n",
        "    X_test  = vectorizer.transform(sentences_test)\n",
        "\n",
        "    classifier = LogisticRegression(solver= 'lbfgs')\n",
        "    classifier.fit(X_train, y_train)\n",
        "    score = classifier.score(X_test, y_test)\n",
        "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfU-L8MPc-db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZnmCALqc-pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "input_dim = X_train.shape[1]  # Number of features\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbQikDWNc_aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVZ7M1TzdUZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    \n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwaPLCDEdp9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(sentences_train[2])\n",
        "print(X_train[2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKv4DZsfd5C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in ['the', 'all', 'me', 'you']:\n",
        "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDSW2j_8d1Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "print(X_train[0, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs-vX_2pEe6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88YN0snneGZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 50\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim= vocab_size, \n",
        "                           output_dim= embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuRR5Q2keJjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDYp6kHreO5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim,\n",
        "                           \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIyAlWIBysvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-cZ36A-vtTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! head -n 1 glove.6B.50d.txt | cut -c-50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbd7JBRSvzxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "    'data/glove.6B.50d.txt',\n",
        "    tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2jA-SW1GBIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "! ls cd data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLWLk3nrwFI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre-trained word embeddings\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUbjP-fWwMua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trainable embeddings layer\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim,\n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen,\n",
        "                           trainable=True))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMmBxdHQwmj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ZcnpCywyfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[5000], \n",
        "                  embedding_dim=[50],\n",
        "                  maxlen=[100])\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = 'data/output.txt'\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "for source, frame in df.groupby('source'):\n",
        "    print('Running grid search for data set :', source)\n",
        "    sentences = df['sentence'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    # Tokenize words\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "    # Adding 1 because of reserved 0 index\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Pad sequences with zeros\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "    model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate testing set\n",
        "    test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "    # Save and evaluate results\n",
        "    prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
        "    if prompt.lower() not in {'y', 'true', 'yes'}:\n",
        "        break\n",
        "    with open(output_file, 'a') as f:\n",
        "        s = ('Running {} data set\\nBest Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "        output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "        print(output_string)\n",
        "        f.write(output_string)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}